import numpy as np
from activation_fns import *

# Simple data for learing even vs. off. 1=odd, 2=even

X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
Y = [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2]



def nn_first(X, Y, lr, acceptable_error):

    #loop until error rate becomes acceptibly low

    # Input layer


    # Hidden layer


    # Ouput layer

    # Compute error (prediction-actual) (get real fn)

    # Compute gradient wi for all weight from hidden layer to output layer then back propogation

    # Compute gradient wi for all weights from input later to hidden layer then  Back propogation

    # update neural network weights 

    # Return neural network

    return()

    